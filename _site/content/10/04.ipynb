{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 这一节我们主要学习\n",
    "+ 深度强化学习的基础知识及其应用场景\n",
    "+ 用基于策略的方法（Policy-based）学习一个做事的 Actor\n",
    "+ 用基于价值的方法（Value-based）学习一个批评的 Critic(下学期内容)\n",
    "+ 将 Actor 与 Critic 结合得到当前最强的方法 A3C（下学期内容）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.深度强化学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "![](http://imgbed.momodel.cn/39-1 deep.png)\n",
    "\n",
    "David Sivler 说 人工智能（AI）= 强化学习（RL）+ 深度学习（DL）\n",
    "\n",
    "15年 Google 在 nature 发表 深度强化学习玩 Atari 游戏的论文\n",
    "\n",
    "16年著名的 Alpha Go 痛扁人类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 强化学习应用场景\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![](http://imgbed.momodel.cn/39-2  scenario.png)\n",
    "有个傻傻的机器人小白（ Agent ）去闯荡世界（ Environment ），世界是非常开放的，将自己的状态（ State ）毫不吝啬地给小白呈现 ，<br>\n",
    "而小白也会做出一些懵懵懂懂的探索动作（ Action ），这时候世界就会告诉小白你的所作所为是好的还是不好的（ Reward ）。<br>\n",
    "小白看到一杯水（ State ），懵懂的小白一下子将它打翻了（ Action ），则他会收到负面反馈（ Reword ）。由于环境是连续的，<br>\n",
    "紧接着小白面前的就是一杯被打翻的水（ State ），于是试着把水擦干净（ Action ），得到了正面反馈（ Reward ）。<br>\n",
    "\n",
    "\n",
    "![](http://imgbed.momodel.cn/39-3 reward.png)\n",
    "\n",
    "于是，小白要做的就是，根据前面收获的正面和负面反馈，去学习哪些能时正面反馈最大化的行为。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 监督学习与增强学习\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![](http://imgbed.momodel.cn/39-4 AlphaGo.png)\n",
    "在下围棋的过程中，环境为你的对手，机器观察棋盘上的落子情况，\n",
    "\n",
    "根据对手的落子，机器做出不同的动作。\n",
    "\n",
    "![](http://imgbed.momodel.cn/39-5 alphago2.png)\n",
    "问题的难点在于，不是每次落子都能够得到有效的 reward，需要结束一盘棋局才能得到 reward，大多数情况下，reward 的值为 0。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "监督学习根据棋谱来进行学习。\n",
    "\n",
    "强化学习是让两个 agent 进行大量的相互对弈，依据经验来进行学习。\n",
    "\n",
    "![](http://imgbed.momodel.cn/39-6 supervised reinforce.png)\n",
    "+ Supervised <br>\n",
    "就是告诉机器说看到什么样的态势就落在指定的位置。<br>\n",
    "Supervised不足的地方就是具体态势下落在哪个地方是最好的，其实人也不知道，因此不太容易做Supervised。<br>\n",
    "用Supervised就是machine从老师那学，老师说下哪就下哪。\n",
    "<br>\n",
    "+ Reinforcement \n",
    "就是让机器找一个对手不断下下，赢了就获得正的reward，没有人告诉它之前哪几步下法是好的，<br>\n",
    "它要自己去试，去学习。Reinforcement 是从过去的经验去学习，没有老师告诉它什么是好的，什么是不好的，machine要自己想办法，<br>\n",
    "其实在做Reinforcement 这个task里面，machine需要大量的training，可以两个machine互相下。<br>\n",
    "\n",
    "Alpha Go 结合了两种方法，先进行监督学习，获得较好的表现之后再进行强化学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 聊天机器人\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement Learning 也可以被用在Learning a chat-bot。<br>\n",
    "\n",
    "chat-bot 是seq2seq，input 就是一句话，output 就是机器的回答。<br>\n",
    "\n",
    "如果采用Supervised ，就是告诉机器有人跟你说“hello”，你就回答“hi”。<br>\n",
    "如果有人跟你说“bye bye”，你就要说“good bye”。<br>\n",
    "\n",
    "如果是Reinforcement Learning 就是让机器胡乱去跟人讲话，讲讲，人就生气了，<br>\n",
    "machine就知道一句话可能讲得不太好。不过没人告诉它哪一句话讲得不好，它要自己去发掘这件事情。<br>\n",
    "\n",
    "![](http://imgbed.momodel.cn/39-8 sequence to sequence.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用强化学习的一种方法是让 Agent 和人对话，Agent 会随机回答，最终可能得到很不好的结果，机器再根据 reward 来调整。\n",
    "\n",
    "![](http://imgbed.momodel.cn/39-7 chatbot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在进行强化学习时，机器要进行大量的对话，一般采取的方法是，\n",
    "\n",
    "+ 让两个机器人互相交谈（有时会产生良好的对话，有时会产生不良影响）。\n",
    "\n",
    "![](http://imgbed.momodel.cn/39-10 agent to.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 通过这种方法，我们可以生成很多对话\n",
    "+ 使用一些预定义的规则来评估对话的优点\n",
    "\n",
    "\n",
    "![](http://imgbed.momodel.cn/39-11 pre-defined rule.png)\n",
    "两个chat-bot互相对话，对话之后有人要告诉它们它们讲得好还是不好。<br>\n",
    "在围棋里比较简单，输赢是比较明确的，对话的话就比较麻烦，<br>\n",
    "你可以让两个machine进行无数轮互相对话，<br>\n",
    "问题是你不知道它们这聊天聊得好还是不好，这是一个待解决问题。<br>\n",
    "现有的方式是制定几条规则，如果讲得好就给它positive reward ，<br>\n",
    "讲得不好就给它negative reward，好不好由人主观决定，<br>\n",
    "然后machine就从它的reward中去学说它要怎么讲才是好。<br>\n",
    "后续可能会有人用GAN的方式去学chat-bot。通过discriminator判断是否像人对话，<br>\n",
    "两个agent就会想骗过discriminator，即用discriminator自动认出给reward的方式。 <br>\n",
    "Reinforcement Learning 有很多应用，尤其是人也不知道怎么做的场景非常适合。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 交互式搜索\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://imgbed.momodel.cn/39-12 interactive retrival.png)\n",
    "让machine学会做Interactive retrieval，<br>\n",
    "意思就是说有一个搜寻系统，能够跟user进行信息确认的方式，<br>\n",
    "从而搜寻到user所需要的信息。<br>\n",
    "直接返回user所需信息，它会得到一个positive reward，然后每问一个问题，都会得到一个negative reward。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Reinforcement Learning 还有很多应用，比如开个直升机，开个无人车呀，<br>\n",
    "也有通过deepmind帮助谷歌节电，也有文本生成等。<br>\n",
    "现在Reinforcement Learning最常用的场景是电玩。<br>\n",
    "现在有现成的environment，比如Gym,Universe。<br>\n",
    "让machine 用Reinforcement Learning来玩游戏，<br>\n",
    "跟人一样，它看到的东西就是一幅画面，就是pixel，然后看到画面，<br>\n",
    "它要做什么事情它自己决定，并不是写程序告诉它说你看到这个东西要做什么。需要它自己去学出来。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![](http://imgbed.momodel.cn/39-13 play video game.png)\n",
    "机器像人类一样学习如何玩游戏，\n",
    "\n",
    "+ 机器观察游戏画面\n",
    "\n",
    "+ 机器学习采取合适的动作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 太空入侵者游戏\n",
    "\n",
    "\n",
    "![](http://imgbed.momodel.cn/39-14 space invader.png)\n",
    "游戏得分为 reward；当所有的外星人被杀光或者飞船被毁游戏结束。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "![](http://imgbed.momodel.cn/39-15 action1.png)\n",
    "动作$a_1$：左移, reward为 0；当执行完动作 $a_1 $之后，外星人也进行了一些移动；\n",
    "\n",
    "但这种变化与机器采取的动作是没有关系的，有时候环境的变化是纯粹随机的。\n",
    "\n",
    "动作 $a_2$：开火，reward 为 5。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经过多轮的循环之后，游戏结束（飞船被摧毁）；这一过程被称为一个\n",
    "\n",
    "episode，我们的学习目标是，在每一轮的 episode 中，最大化累积的 reward。\n",
    "\n",
    "\n",
    "![](http://imgbed.momodel.cn/39-16 episode.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 强化学习的难点\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 奖励延迟\n",
    "\n",
    "  + 在太空入侵的游戏中，仅仅开火这一个动作能够获得奖励，尽管开火前的移动也很重要\n",
    "  \n",
    "  + 在下围棋时，牺牲短期的好处以获得长足的利益可能才是更好的选择\n",
    "  \n",
    "+ 机器的操作会影响其接受的后续数据\n",
    "\n",
    "     + 机器要能够探索他没有做过的行为"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 强化学习的方法\n",
    "\n",
    "\n",
    "Reinforcement Learning 的方法主要分为Policy-based的方法和 Valued-based 的方法。<br>\n",
    "先有Valued-based的方法，再有Policy-based的方法。<br>\n",
    "在Policy-based的方法里面，会learn一个负责做事的Actor，<br>\n",
    "在Valued-based的方法会learn一个不做事的Critic，专门批评不做事的人。<br>\n",
    "我们要把Actor和Critic加起来叫做Actor+Critic的方法。<br>\n",
    "\n",
    "\n",
    "#### 2.1 用基于策略的方法（Policy-based Approach）学习一个 Actor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "![](http://imgbed.momodel.cn/39-17 look for a function.png)    \n",
    "先来看看怎么学一个Actor:<br>\n",
    "所谓的Actor是什么呢?我们之前讲过，Machine Learning 就是找一个Function，<br>  \n",
    "Reinforcement Learning也是Machine Learning 的一种，所以要做的事情也是找Function。<br>\n",
    "这个Function就是所谓的魔术发现，Actor就是一个Function。<br>\n",
    "这个Function的input就是Machine看到的observation，它的output就是Machine要采取的Action。<br>\n",
    "我们要透过reward来帮我们找这个best Function。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "找个这个Function有三个步骤：\n",
    "\n",
    "\n",
    "第一个步骤就是决定你的Function长什么样子，假设你的Function是一个Neural Network，就是一个deep learning。\n",
    "\n",
    "![](http://imgbed.momodel.cn/39-18 steps.png)\n",
    "如果Neural Network作为一个Actor，这个Neural Network的输入就是observation，<br>\n",
    "可以通过一个vector或者一个matrix 来描述。<br>\n",
    "output就是你现在可以采取的action。<br>\n",
    "举个例子，Neural Network作为一个Actor，inpiut是一张image，<br>\n",
    "output就是你现在有几个可以采取的action，output就有几个dimension。<br>\n",
    "假设我们在玩Space invader，output就是可能采取的action左移、右移和开火，<br>\n",
    "这样output就有三个dimension分别代表了左移、右移和开火。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://imgbed.momodel.cn/39-19 nn as actor.png)\n",
    "神经网络的输入为机器的 observation，即像素点组成的向量或矩阵，输出对应机器每一个动作的发生概率。\n",
    "\n",
    "相比于传统的查找表，神经网络有**<span>泛化</span>**的功能，不用穷举所有输入的情况"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "第二步骤就是，我们要决定一个Actor的好坏。<br>\n",
    "在Supervised learning中，我们是怎样决定一个Function的好坏呢？<br>\n",
    "举个Training Example例子来说，我们把图片扔进去，看它的结果和target是否像，<br>\n",
    "如果越像的话这个Function就会越好，<br>\n",
    "我们会一个loss，然后计算每个example的loss，我们要找一个参数去minimize这个参数。<br>\n",
    "\n",
    "![](http://imgbed.momodel.cn/39-20 goodness.png)\n",
    "在分类问题中，我们使用交叉熵作为损失函数，我们需要找到最佳的参数 $\\theta^*$ 使损失函数最小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "![](http://imgbed.momodel.cn/39-21 godness of actor.png)\n",
    "在强化学习中，我们使用总奖励的期望值 $\\bar R_{\\theta}$ ，来评估一个 Actor $\\pi_{\\theta}(s)$ 的好坏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![](http://imgbed.momodel.cn/39-22 godness2.png)\n",
    "我们使用 $\\pi_{\\theta}(s)$ 进行 N 次游戏，用 $R_{\\theta}$ 的平均值来代替期望 $\\bar R_{\\theta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "怎么选择最好的function，其实就是用我们的Gradient Ascent。我们已经找到目标了，就是最大化这个$\\bar{R}_\\theta$\n",
    "\n",
    "然后用梯度下降的方法找到最佳的 Actor：\n",
    "\n",
    "+ 问题描述：\n",
    "  \n",
    "  $\\theta^* = arg max \\bar R_{\\theta}$     $\\bar R_{\\theta}$ = $\\sum_{\\tau} R(\\tau)P(\\tau|\\theta)$ \n",
    "\n",
    "<br>\n",
    "  \n",
    "+ 梯度下降：\n",
    "\n",
    "  \n",
    "![](http://imgbed.momodel.cn/39-23 gradient descent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "因为我们只能改变 $\\theta$ 来改变 Actor,所以只对 $\\theta$ 求导。\n",
    "\n",
    "![](http://imgbed.momodel.cn/39-24 gd2.png)\n",
    "这里我们同样需要使用样本的概率来代替 $P(\\tau|\\theta)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ $P(\\tau|\\theta)$ 的计算：\n",
    "\n",
    "其中有些概率项和 Actor 无关，有些与 Actor 有关\n",
    "\n",
    "\n",
    "![](http://imgbed.momodel.cn/39-25  p.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 计算$\\nabla$ $\\log{P(\\tau|\\theta)}$：\n",
    "\n",
    "忽略掉与 $\\theta$ 无关的部分\n",
    "\n",
    "\n",
    "![](http://imgbed.momodel.cn/39-26 nabla logp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最终的计算结果：\n",
    "\n",
    "\n",
    "![](http://imgbed.momodel.cn/39-27 answer.png)\n",
    "当 R 为正时，调整 $\\theta$ 的值，来增大某一个动作发生的概率。\n",
    "\n",
    "当 R 为负时，调整 $\\theta$ 的值，来减小某一个动作发生的概率。\n",
    "\n",
    "\n",
    "![](http://imgbed.momodel.cn/39-28 tune.png)\n",
    "我们使用累计的 reward 而不是及时的奖励。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![](http://imgbed.momodel.cn/39-29 division.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 1.当 $R_{\\theta}$ 一直为正时，并不会影响那些 reward 值较大的动作；<br>\n",
    "   因为最后神经网络要经过 softmax 来输出每个动作的概率，那些 reward 较小<br>\n",
    "   的动作，发生的概率也会相应减小。\n",
    "   \n",
    "+ 2.在采样的过程中，我们可能会遇到某一个 action 不被采样的情况，这时我们需要添加<br>\n",
    "  一个 baseline，只有 reward 超过 baseline 的动作发生的概率才会增加。这样某些动作<br>\n",
    "  不被采样的概率将会减小。\n",
    "\n",
    "![](http://imgbed.momodel.cn/39-30 baseline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 用基于价值的方法（Value-based Approach）学习一个 Critic\n",
    "\n",
    "\n",
    "Critic就是Learn一个Neural Network，这个Neural Network不做事，然后Actor可以从这个Critic中获得，这就是Q-learning。 <br>\n",
    "Critic就是learn一个function，这个function可以告诉你说现在看到某一个observation的时候，这个observation有有多好这样。 <br>\n",
    "\n",
    "+ 根据actor π 评估critic function\n",
    "\n",
    "这个function是用Neural Network表示\n",
    "\n",
    "+ state value function $V^\\pi(s)$\n",
    "\n",
    "这个累加的reward是通过观察多个observation\n",
    "\n",
    "![](http://imgbed.momodel.cn/chapter39_15.png)\n",
    "那么如何估计 $V^\\pi(s)$ 呢？可以采用Monte-Carlo based approach。\n",
    "\n",
    "+ State-action value function $Q^π(s,a)$\n",
    "\n",
    "这个累加的reward是通过观察observation和take的action\n",
    "\n",
    "![](http://imgbed.momodel.cn/chapter39_16.png)\n",
    "#### 2.3 Actor-Critic\n",
    "\n",
    "![](http://imgbed.momodel.cn/chapter39_17.png)    \n",
    "该部分在李宏毅课程的第二个学期中介绍：https://www.bilibili.com/video/av35757082/?p=33"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
