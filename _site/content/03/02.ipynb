{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 这一节我们主要学习\n",
    "+ 逻辑回归模型\n",
    "+ 逻辑回归与平方误差\n",
    "+ 判别模型 vs. 生成模型\n",
    "+ 多元分类\n",
    "+ 逻辑回归的局限性\n",
    "+ 延伸：深度学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. 建立逻辑回归模型**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 步骤1：**确定函数集**\n",
    "\n",
    "  我们需要找到一个概率函数 $P_{w,b}(C_1|x)$  ，该函数有如下特征：\n",
    "  \n",
    "  当 $P_{w,b}(C_1|x) \\geq 0.5$ &nbsp;时，输出 $C_1$ ； 否则输出 $C_2$。\n",
    "  \n",
    "  函数 $P_{w,b}(C_1|x) = σ(z)$ ， 满足上述条件。\n",
    "  \n",
    "  其中$z = wx + b，σ(z) = \\frac{1}{1+exp(-z)}$ ，如图。\n",
    "    \n",
    "![](http://imgbed.momodel.cn/10-1 sigmod函数图像.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "  函数集 $f_{w,b}(x) = P_{w,b}(C_1|x)$  ，如图：\n",
    "  \n",
    "  \n",
    "![](http://imgbed.momodel.cn/10-2 函数集示意图.png)  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 比较逻辑回归和线性回归：\n",
    "  \n",
    "  + 逻辑回归：$f_{w,b}(x) = σ(\\sum_{i}w_ix_i+b)$ ； 输出范围(0 , 1)\n",
    "  \n",
    "  + 线性回归：$f_{w,b}(x) = \\sum_{i}w_ix_i+b$ ；输出范围($-\\infty$,$+\\infty$)\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 步骤2：**模型评价**\n",
    "<span class='md-video-link https://player.bilibili.com/player.html?aid=10590361&cid=17482193&page=11'>模型评价 - 3m54s</span>**\n",
    "\n",
    "  如图所示，假设训练数据集是依据 $f_{w,b}(x) = P_{w,b}(C_1|x)$ 产生的，给出一系列的 $w$ 和 $b$，产生该数据集的概率为多少？\n",
    "  \n",
    "  \n",
    "![](http://imgbed.momodel.cn/10-3 训练数据示意图.png)  \n",
    "  \n",
    "  \n",
    "  产生该数据集的概率为 $L(w,b) = f_{w,b}(x^1)f_{w,b}(x^2)(1-f_{w,b}(x^3)) \\cdots f_{w,b}(x^N)$\n",
    "      （注意$x^3$属于$C_2$类别）\n",
    "  \n",
    "  其中 $w^*,b^* = arg maxL(a,b)$，当 $w$ 和 $b$ 分别取 $w^*,b^* $ 时，函数 $L(w,b)$ 取最大值。\n",
    "  \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ** $L(w,b)$的优化：**\n",
    " \n",
    "![](http://imgbed.momodel.cn/10-4 标签y的取值.png)  \n",
    "  $L(w,b) = f_{w,b}(x^1)f_{w,b}(x^2)(1-f_{w,b}(x^3)) \\cdots $\n",
    "  \n",
    "  为简化计算，我们将求解 $L(w,b)$ 的最大值，转化为求解 $-lnL(w,b)$ 的最小值。\n",
    "  \n",
    "  \n",
    "![](http://imgbed.momodel.cn/10-5 简化示意图.png)  \n",
    "  $-lnL(w,b)= \\ln{f_{w,b}(x^1)}\\ln{f_{w,b}(x^2)}\\ln{(1-f_{w,b}(x^3))} \\cdots$ \n",
    "  = $\\sum_{n=1}^{n=N} -[\\hat y^{n}\\ln{f_{w,b}(x^n)}+(1-\\hat y^{n})\\ln({1-f_{w,b}(x^n)})]$,\n",
    "  \n",
    "  该函数可以看做两个伯努利分布之间的交叉熵，如图。\n",
    "  \n",
    "  \n",
    "![](http://imgbed.momodel.cn/10-6 交叉熵示意图.png)  \n",
    "  熵的概念本来是热力学的一个概念，描述物质的混乱程度。 在这里，我们用交叉熵的概念来描述两组不同概率数据分布的相似程度，越小越相似。\n",
    "  \n",
    "  + **逻辑回归和线性回归训练集上的比较：**\n",
    "  \n",
    "  训练数据集($x^n,\\hat y^{n}$)：<br>\n",
    "  \n",
    "  逻辑回归：$L(f) = \\sum C(f(x^n),\\hat y^{n})=\\sum-[\\hat{y}^nlnf(x^n)+(1-\\hat{y}^n)ln(1-f(x^n))]$  其中：$\\hat y^{n}$ 取1表示分类1，取2表示分类2；\n",
    "  \n",
    "  线性回归：$L(f) = \\frac{1}{2}\\sum (f(x^n)-\\hat y^{n})^2$ ；其中$\\hat y^{n}$ 表示一个真实有实际意义的值。\n",
    "  \n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 步骤3：**模型优化**\n",
    "\n",
    "  偏导数： $\\frac{\\partial (-lnL(w,b))}{\\partial w_i}$ = $\\sum_n-(\\hat y^{n}-f_{w,b}(x^n))x_i^n$\n",
    "  \n",
    "  梯度下降：$w_i \\leftarrow w_i - 𝜂\\sum_n -(\\hat y^n - f_{w,b}(x^n))x_i^n$，由上式可以看出，偏差越大，更新的步长越大。\n",
    "  \n",
    "  **逻辑回归与线性回归在参数更新上的比较：**\n",
    "  \n",
    "  梯度下降表达式均为 **$w_i \\leftarrow w_i - 𝜂\\sum_n -(\\hat y^n - f_{w,b}(x^n))x_i^n$**。\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 **逻辑回归与回归模型总结对比**\n",
    "  \n",
    "![](http://imgbed.momodel.cn/10-8 总结.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. 逻辑回归与平方误差**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ **步骤1**：\n",
    "\n",
    "$f_{w,b}(x) = σ(\\sum_i w_ix_i+b)$\n",
    "\n",
    "对线性回归中的加速函数 $h$ 作 sigmod 函数得到逻辑回归假设函数。\n",
    "\n",
    "+ **步骤2**：\n",
    "\n",
    "训练数据集($x^n,\\hat y^n$)：当 $\\hat y^n$ 为类别1时，取1；为类别2时，取0 。\n",
    "\n",
    "$L(f)=\\frac{1}{2}\\sum_n(f_{w,b}(x^n)-\\hat y^n)^2$\n",
    "\n",
    "+ **步骤3**：\n",
    "\n",
    "$\\frac{\\partial (f_{w,b}(x^n)-\\hat y)^2}{\\partial w_i}$ = $2(f_{w,b}(x^n)-\\hat y^n)f_{w,b}(x^n)(1-f_{w,b}(x^n))x_i$\n",
    "       \n",
    "$\\hat y^n$ = 0时，\n",
    "\n",
    "如果 $f_{w,b}(x^n)$ = 1(远离目标)，$\\frac{\\partial L}{\\partial w_i }= 0$\n",
    "\n",
    "如果 $f_{w,b}(x^n)$ = 0(靠近目标)，$\\frac{\\partial L}{\\partial w_i }= 0$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### **3. 判别模型 v.s. 生成模型 **       \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.1 介绍**\n",
    "\n",
    "我们把逻辑回归方法称为判别模型（ Discriminative ）方法，\n",
    "\n",
    "上节课用高斯方法来描述后验概率的方法称为生成模型（ Generative ）方法。\n",
    "\n",
    "概率函数 $P_{w,b}(C_1|x)$ ，通过两种不同的方式求解 $w,b$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 1 直接求解 $w,b$\n",
    "\n",
    "+ 2 计算 $μ^1$，$μ^2$ ，$\\sum^{-1}$ ，再通过下式计算：\n",
    "\n",
    "\n",
    "![](http://imgbed.momodel.cn/10-9 生成模型的参数求解.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "如果使用生成模型，可以使用方法2计算。\n",
    "\n",
    "它们使用相同的训练数据和函数集进行训练，但是最终的得到的预测函数不相同，如图所示。\n",
    "\n",
    "\n",
    "![](http://imgbed.momodel.cn/10-10 预测模型和生成模型对比.png)\n",
    "上述两种模型采用了数据集的7种数据特征来对数据集进行分类，其中**生成模型**的准确率为73%，**判别模型**的准确率为79%。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.2 生成模型实例**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "已知特征均为1时，是类别1，其余为类别2。\n",
    "\n",
    "![](http://imgbed.momodel.cn/10-11 例子.png)\n",
    "我们来预测一下：\n",
    "\n",
    "![](http://imgbed.momodel.cn/20190530233132.png)\n",
    "如果使用贝叶斯模型（生成模型）会得到什么样的结果？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://imgbed.momodel.cn/10-13 贝叶斯概率求解1.png)\n",
    "![](http://imgbed.momodel.cn/10-14 贝叶斯求解2.png)\n",
    "预测结果为类别2，应该是类别1的，为什么出错了呢？\n",
    "\n",
    "生成模型经过计算判断测试数据是类别1的几率小于0.5，原因在于生成模型会适当脑补一些情形。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. 多元分类**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类比二元分类，多元分类即根据特征将数据分为多个类别。\n",
    "\n",
    "$C_1:w^1,b^1$;$z_1 = w^1*x + b_1$\n",
    "\n",
    "$C_2:w^2,b^2$;$z_2 = w^2*x + b_2$\n",
    "\n",
    "$C_3:w^3,b^3$;$z_3 = w^3*x + b_3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Probability**：$y_i = P(C_i|x)$\n",
    "+ $1 > y_i >0$\n",
    "\n",
    "+ $\\sum_i y_i=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**使用 softmax 函数**：\n",
    "\n",
    "![](http://imgbed.momodel.cn/10-15 softmax.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**softmax 函数和 交叉熵（ cross entropy ）:**\n",
    "\n",
    "熵的概念本来是热力学的一个概念，描述物质的混乱程度。 \n",
    "\n",
    "在这里，我们用交叉熵的概念来描述两组不同概率数据分布的相似程度，越小越相似。\n",
    "\n",
    "![](http://imgbed.momodel.cn/10-16softmax和crossentropy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. 逻辑回归的局限性**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **“异或”函数**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://imgbed.momodel.cn/10-17 异或函数.png)\n",
    "\n",
    "如图，我们不能够将数据集进行很好的分类。请问为什么呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **解决办法**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **特征变化：**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://imgbed.momodel.cn/10-18 特征变换.png)\n",
    "如图，我们通过一些变换将数据在坐标轴上分开，但是我们并不能很容易找到一个较好的变换。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **级联逻辑回归模型**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "![](http://imgbed.momodel.cn/10-19 级联逻辑回归模型.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先通过特征变换，改变每个点的特征值：\n",
    "\n",
    "\n",
    "\n",
    "![](http://imgbed.momodel.cn/10-20 第一级变换.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过第二级模型进行分类：\n",
    "\n",
    "\n",
    "![](http://imgbed.momodel.cn/10-21 第二级变化.png)\n",
    "\n",
    "如图，我们成功地将数据进行分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6. 深度学习**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "![](http://imgbed.momodel.cn/10-22 深度学习.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
