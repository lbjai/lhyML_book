{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 这一节我们主要学习\n",
    "\n",
    "+ 为什么要使用词嵌入（Word Embedding）\n",
    "+ 为什么词嵌入是无监督学习\n",
    "+ 基于统计（Count based ）\n",
    "+ 基于预测（Predition based）\n",
    "+ 词嵌入的特点\n",
    "+ 词嵌入的主要应用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 为什么要使用词嵌入\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "在词嵌入之前往往采用 1-of-N Encoding 的方法，如下图所示 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://imgbed.momodel.cn/25_1_ul_we.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这种方法主要有两个缺点，首先这种表示是正交的，但是正是由于正交性使得具有相似属性的词之间的关系变得微弱；<br>\n",
    "其次这种编码方式使得词向量很长，比如说一共有10万个单词，那么就需要一个长度为10万的词向量进行编码。<br>\n",
    "<br>\n",
    "为了克服这样的缺点，采用了词嵌入的方法。这种方法将词映射到高维之中（但是维数仍比 1-of-N Encoding 低很多），<br>\n",
    "相似的单词会聚集在一起，而不同的单词会分开；每个坐标轴可以看作是区分这些单词的一种属性，<br>\n",
    "比如说在上图中，横坐标可以认为是生物与其他的区别，纵坐标可以认为是会动和不会动的区别。<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 为什么词嵌入是无监督学习\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为在进行学习的过程中，我们只知道输入的是词的一种编码，输出的是词的另一种编码，但是并不知道具体应该是怎样的一种编码，所以是无监督学习。<br> \n",
    "有的人可能想可以通过自编码的方式实现词嵌入，但是如果你得输入是 1-of-N Encoding 的话，是基本上不可以采用这样的方法处理的 ，因为输入的向量都是无关的，很难通过自编码的过程提取出什么有用的信息。<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**词嵌入**的两种方式:<br>\n",
    "词嵌入 主要有**基于统计 （Count based ）** 和**基于预测 （Predition based）** 的两种方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 基于统计（Count based ）<br>\n",
    "<br> \n",
    "这种方法的主要思路如下图所示 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://imgbed.momodel.cn/25_2_ul_we.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "两词向量共同出现的频率比较高的话，那么这两个词向量也应该比较相似。<br>\n",
    "所以两个词向量的点积应该与它们公共出现的次数成正比，这与上节课讲的矩阵分解就很像了。<br>\n",
    "具体可以参考：Glove Vector : http://nlp.stanford.edu/projects/glove/<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 基于预测（Prediction based）\n",
    "用前一个词预测后一个词："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 词预测训练过程\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://imgbed.momodel.cn/25_3_ul_we.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "中文中“潮”、“水”是一个字，而“潮水”是一个词。<br>\n",
    "中文是以词和字为单位进行分词处理，故对中文如何分词是十分重要的。<br>\n",
    "但是对中文有一个断词问题，就是一个句子有好几种不同的分断法。<br>\n",
    "其作用可以拿来做推文接话，如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://imgbed.momodel.cn/25_4_ul_we.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上图中红色部分都是一个词接一个词预测出来，然后形成一句话。<br>\n",
    "推文接话参考：https://www.ptt.cc/bbs/Teacher/M.1317226791.A.558.html<br>\n",
    "预测的另外一个作用：Language Modeling<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://imgbed.momodel.cn/25_5_ul_we.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预测一个句子出现的几率，通常采用估测该句子中首个单词接下来的单词概率（比如通过 wreck 得到 a 的概率），<br>\n",
    "依次类推最终得到句子的概率等于所有单词的概率的乘积。<br>\n",
    "比如图中p(wrek|START)p(a|wreck)p(nice|a)p(beach|nice)<br>\n",
    "主要应用在机器翻译、语音识别中。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://imgbed.momodel.cn/25_6_ul_we.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 上图主要表达：预测$w_t$出现的几率，<br>\n",
    "    输入是：$w_{t-1},w_{t-2},...,w_{t-n+1}$<br>\n",
    "   每一个输入都会乘上一个矩阵C，成为$C(w_{t-1})，C（w_{t-2}）,...,C(w_{t-n+1})$<br>\n",
    "   然后将上述结果并起来并通过 tanh 激活函数和 softmax 处理得到某个单词出现的几率。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 基于预测的原理 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://imgbed.momodel.cn/25_7_ul_we.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先对词进行 one-hot 编码，然后进行类似于降维的处理，<br>\n",
    "得到第一个隐藏层的输入 $z_1,z_2,...$<br>\n",
    "训练出神经网络，所以我们将它的第一个隐藏层拿出来，就能将它们对应到相应的空间。<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://imgbed.momodel.cn/25_8_ul_we.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用第一个隐藏层的输出代表该词的词向量。（图中绿色的一层）<br>\n",
    "例子中“蔡英文”、“马英九”要让它们在进入隐藏层之后输入相近（为了得出共同结果“宣誓就职”）。<br>\n",
    "而且，仅通过一个词汇就要预测下一个词汇是很难的，所以通过共享参数来进行增强。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 共享参数（Sharing Parameters）\n",
    "\n",
    "\n",
    "不仅用前一个词，还用前 n 个词来一起预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://imgbed.momodel.cn/25_9_ul_we.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样的权重是为了让同一个词放在 i-1 的位置和 i-2 的位置都有同样的输出，或者两个近义词可以得到近似的词向量输出，另外的好处是可以减少参数量。 <br>\n",
    "计算过程："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://imgbed.momodel.cn/25_10_ul_we.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 基于预测的其他结构\n",
    "\n",
    "\n",
    "\n",
    "用前两个词预测后一个词和通过中间词汇预测前后2个词汇"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://imgbed.momodel.cn/25_11_ul_we.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 词嵌入的特点\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://imgbed.momodel.cn/25_12_ul_we.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://imgbed.momodel.cn/25_13_ul_we.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "国家与首都有着固定的关系、动词的三态变化是一个有趣的三角形。<br>\n",
    "可以发现，我们把同样类型的单词摆在一起，他们之间是有固定的关系的。<br>\n",
    "比如让两个单词两两相减，然后部署到一个空间上，如果落到同一处，则他们之间的关系是很类似的。<br>\n",
    "参考资料： http://www.slideshare.net/hustwj/cikm-keynotenov2014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 应用\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 多语种词向量 （Multi-lingual Embedding）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://imgbed.momodel.cn/25_14_ul_we.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "中文的 word embedding 和英文的 word embedding 无法将其混在一起使用。<br>\n",
    "但是将一些中文英文对应的词放在一起训练后，黄色部分词汇靠近其相近意思的中文词汇附近。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 文章 Embedding（Document Embedding）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://imgbed.momodel.cn/25_15_ul_we.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://imgbed.momodel.cn/25_16_ul_we.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "难点：不同文章长度不一致；不同的文章内容有不同的词汇数目；<br>\n",
    "解决方式：采用 bag-of-word 来描述文章"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://imgbed.momodel.cn/25_17_ul_we.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相同词汇，不同顺序表达不同的意思。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 总结\n",
    "+ 词嵌入的作用和分类\n",
    "+ 基于统计的词嵌入模型原理\n",
    "+ 基于预测的词嵌入模型原理"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
